Results for roberta-base with max_seq_len: 64, learning_rate: 5e-06, batch_size: 128, epochs: 4:
Classification Report:
              precision    recall  f1-score   support

           0       0.85      0.86      0.86       648
           1       0.74      0.72      0.73       352

    accuracy                           0.81      1000
   macro avg       0.79      0.79      0.79      1000
weighted avg       0.81      0.81      0.81      1000


Results for roberta-base with max_seq_len: 64, learning_rate: 5e-06, batch_size: 128, epochs: 4
This model uses a set of offensive words to determine the percentage of offensive words in the tweet and use it as a new feature.:
Classification Report:
              precision    recall  f1-score   support

           0       0.85      0.85      0.85       648
           1       0.72      0.72      0.72       352

    accuracy                           0.80      1000
   macro avg       0.78      0.78      0.78      1000
weighted avg       0.80      0.80      0.80      1000


Results for roberta-base with max_seq_len: 64, learning_rate: 5e-06, batch_size: 128, epochs: 4
This model uses a set of offensive words to determine if the tweet contains any offensive words and use it as a new feature.:
Classification Report:
              precision    recall  f1-score   support

           0       0.86      0.84      0.85       648
           1       0.72      0.74      0.73       352

    accuracy                           0.81      1000
   macro avg       0.79      0.79      0.79      1000
weighted avg       0.81      0.81      0.81      1000


